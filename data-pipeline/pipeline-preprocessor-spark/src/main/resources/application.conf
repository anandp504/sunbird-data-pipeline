
kafka {
  input.topic = "telemetry.raw"
  output.success.topic = "telemetry.valid"
  output.failed.topic = "telemetry.failed"
  output.primary.route.topic = "telemetry.sink"
  output.secondary.route.topic = "telemetry.log"
  output.audit.route.topic = "telemetry.audit"
  output.duplicate.topic = "telemetry.duplicate"
  output.malformed.topic = "telemetry.malformed"
  broker-servers = "172.16.0.153:9092"
  zookeeper = "172.16.0.153:2181"
  groupId = "pipeline-preprocessor-spark-group"
}

task {
  parallelism = 3
  checkpointing.interval = 60000
  microbatch.interval = 10
}

telemetry.schema.path="schemas/telemetry/3.0"
router.secondary.routes.eid = ["LOG", "ERROR"]
default.channel="default_channel"
dedup.producer.included.ids = ["prod.diksha.portal", "prod.sunbird.desktop"]

redis {
  host = 172.16.0.153
  port = 6379
  connection {
    max = 2
    idle.min = 1
    idle.max = 2
    minEvictableIdleTimeSeconds = 120
    timeBetweenEvictionRunsSeconds = 300
  }
  database {
    duplicationstore.id = 7
    key.expiry.seconds = 432000
  }
}